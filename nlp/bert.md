# Pre-training of Deep Bidirectional Transformers for Language Understanding

## Initial Thoughts
* Bidirectional + transformers i.e. LSTMs
* Language representation models - more syntactic in nature?
* Transfer learning - using 1 problem solution to help solve another one. BERT can "create state-of-the-art models for a wide range of tasks, such as question answering and language inference, WITHOUT substantial task-specific architecture modifications." 




# Pre-training of Deep Bidirectional Transformers for Language Understanding

## Initial Thoughts - Abstract
* Bidirectional + transformers i.e. LSTMs
* Language representation models - more syntactic in nature?
* Transfer learning - using 1 problem solution to help solve another one. BERT can "create state-of-the-art models for a wide range of tasks, such as question answering and language inference, WITHOUT substantial task-specific architecture modifications." 
* SQuAD answers middle school questions

## Introduction




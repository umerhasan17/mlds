# Attention is all you need

## Initial Thoughts (Abstract)
* Attention is all you need has no recurrence. Models proposed in this paper are transformers which are doing especially well with machine translation. The other method is to use bidirectional LSTMs.
* "sequence transduction" are sequence to sequence models e.g. translation. 
* Current models are based on RNN/CNN with an encoder and a decoder. The encoder & decoder are connected through an attention mechanism. 
* This paper did not develop attention, they got rid of everything other than attention. 
* http://jalammar.github.io/illustrated-transformer/
* For English to French attention trained for 3.5 days, WaveNet was much longer. RNN have specific properties which make training slower. 
